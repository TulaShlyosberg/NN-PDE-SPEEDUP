# NN-PDE-SPEEDUP

Проект о применимости нейронных сетей для итерационных методов.

Ноутбуки проекта:

**1-jacobi-umap**

Здесь реализован метод Якоби и визулизированы его шаги с помощью метода снижения размерности UMAP. Также показана квазипериодичность этого метода с помощью матрицы расстояний.

**2-dataset-generator**

Здесь реализован класс, генерирующий датасет для метода Якоби; кроме того, предпринята попытка предсказать стационарное решение в лоб.

**3-autoencoder**

Малопримечательный ноутбук, если бы он не содержал работу автоэнкодера (далеко не самого качественного). Скорее стоит говорить, что это мы не смогли ппостроить нормальный автоэнкодер, чем думать, будто в этой задаче он не очень хорош.

**4-rnn-one-and-all**

Сделаны 2 вещи. В первой части ноутбука показано, что можно обучить LSTM на небольшом подмножестве обучения, и нейросеть будет выдавать корректный результат еще некоторое время.

Во второй части взят большой датасет; обучение LSTM проходило на одной части датасета. На второй части мы проверяли, как хорошо LSTM предсказывает шаги метода Якоби по начальным условиям. Результат можно увидеть на гифке:

![](./img/lstm_1.gif "result many")

**5-smoothed-init-cond**

До этого начальные условия были разрывными. После показано, что LSTM дает неплохие результаты в этом случае, гораздо лучше, чем в разрывном. 

![](./img/lstm_2.gif "result one")
